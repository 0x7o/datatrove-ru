from datatrove.data import Document
from datatrove.pipeline.filters.base_filter import BaseFilter
from datatrove.pipeline.filters.gopher_repetition_filter import find_duplicates
from datatrove.pipeline.writers.disk_base import DiskWriter
from datatrove.utils.typeshelper import Languages
from datatrove.utils.word_tokenizers import load_word_tokenizer


class FineWebQualityFilter(BaseFilter):
    name = "üç∑ FineWeb Quality"

    def __init__(
            self,
            exclusion_writer: DiskWriter = None,
            line_punct_thr: float = 0.12,
            line_punct_exclude_zero: bool = False,
            short_line_thr: float = 0.67,
            short_line_length: int = 30,
            char_duplicates_ratio: float = 0.01,
            new_line_ratio: float = 0.3,
            language: str = Languages.english,
    ):
        super().__init__(exclusion_writer)
        self.line_punct_thr = line_punct_thr
        self.line_punct_exclude_zero = line_punct_exclude_zero
        self.short_line_threshold = short_line_thr
        self.short_line_length = short_line_length
        self.char_duplicates_ratio = char_duplicates_ratio
        self.new_line_ratio = new_line_ratio
        self.tokenizer = load_word_tokenizer(language)

    def filter(self, doc) -> bool | tuple[bool, str]:
        stop_chars = (".", "'", '"', "!", "?")

        lines = doc.text.split("\n")
        ratio = sum(1 for line in lines if line.endswith(stop_chars)) / len(lines)
        if ratio <= self.line_punct_thr and not (ratio == 0 and self.line_punct_exclude_zero):
            return False, "line_punct_ratio"

        ratio = sum(1 for line in lines if len(line) <= self.short_line_length) / len(lines)
        if ratio >= self.short_line_threshold:
            return False, "short_line_ratio"

        non_empty_lines = [line for line in lines if line.strip() != ""]
        ratio = find_duplicates(non_empty_lines)[1] / len(doc.text.replace("\n", ""))

        if ratio >= self.char_duplicates_ratio:
            return False, "char_dup_ratio"

        words = self.tokenizer.word_tokenize(doc.text)
        new_line = doc.text.count("\n")
        if new_line / len(words) > self.new_line_ratio:
            return False, "list_ratio"

        return True


if __name__ == "__main__":
    filter = FineWebQualityFilter(language=Languages.russian)
    doc = Document(text='‚úîüëçüèø –ö—É–ø–∏—Ç—å Seaweed Organic Mask –≤ –ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫–µ —Ä–∞–∑–≤–æ–¥ –ö—É–ø–∏—Ç—å Seaweed Organic Mask –≤ –ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫–µ —Ä–∞–∑–≤–æ–¥ –ï–ª–µ–Ω–∞ –ú–∞–ª—ã—à–µ–≤–∞: "–ü–∏–≥–º–µ–Ω—Ç–∞—Ü–∏—è –ª–∏—Ü–∞ –≤ –ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫–µ –æ—Å—Ç–∞–ª–∞—Å—å –≤ –ø—Ä–æ—à–ª–æ–º". –ü–µ—Ä–µ–π—Ç–∏ –Ω–∞ —Å–∞–π—Ç –ø–æ—Å—Ç–∞–≤—â–∏–∫–∞ –≤ –ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫–µ Seaweed Organic Mask –º–∞—Å–∫–∞ –∏–∑ –≤–æ–¥–æ—Ä–æ—Å–ª–µ–π ‚Ä∫ ‚úî‚úî‚úî –≤ –ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫–µ ‚úî‚úî‚úî –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ: 09.09.2017, 20:12 | –ê–≤—Ç–æ—Ä: –î–∞—Ä–∏–Ω–∞ –ú–∞—Ç–≤–µ–µ–≤–∞ –Ø —Ç–æ–∂–µ —Ä–µ—à–∏–ª–∞ –∑–∞–∫–∞–∑–∞—Ç—å.–ö–æ–≥–¥–∞ –≥–ª—è–Ω—É–ª–∞ –Ω–∞ —Å—Ç–∞—Ç—å—é, —Å—Ä–∞–∑—É –ø–æ–Ω—è–ª–∞ —á—Ç–æ –ó–∞–∫–∞–∑—ã–≤–∞–ª–∞ –ó–î–ï–°–¨. –ß—Ç–æ —ç—Ç–æ—Ç —Ç–æ—Ç –∂–µ —Å–∞–π—Ç. –û –∫–æ—Ç–æ—Ä–æ–º –∏ –≤—Ä–∞—á –≥–æ–≤–æ—Ä–∏–ª. –í–æ—Ç —Ç–æ–ª—å–∫–æ —Å—Ä–æ–∫–∏ –¥–æ —ç—Ñ—Ñ–µ–∫—Ç–∞ —É –º–µ–Ω—è –¥—Ä—É–≥–∏–µ, –Ω–µ –∫–∞–∫ —É –ï–ª–µ–Ω—ã, –Ω–µ –∑–Ω–∞—é —Å —á–µ–º —ç—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–≤—è–∑–∞–Ω–æ.–ö–æ–∂–∞ –æ—á–∏—Å—Ç–∏–ª–∞—Å—å —É–∂ —Ç–æ—á–Ω–æ –Ω–µ –∑–∞ –º–µ—Å—è—Ü, —Ç–æ—á–Ω–æ –ø–æ–º–Ω—é, –ø–∏–≥–º–µ–Ω—Ç–∞—Ü–∏—è –±—ã–ª–∞ –≤ —Ä–∞–∑—ã —Å–∏–ª—å–Ω–µ–µ, —Ç–∞–∫ —á—Ç–æ –Ω–µ—Ç, –Ω–µ –º–µ—Å—è—Ü. –ê –ø—Ä–∏–º–µ—Ä–Ω–æ 6 –Ω–µ–¥–µ–ª—å. –ó–∞—Ç–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Å—Ç–æ–∏–ª–æ –∂–¥–∞—Ç—å, –æ–Ω –ø—Ä–æ—Å—Ç–æ –Ω–µ–≤–µ—Ä–æ—è—Ç–µ–Ω. –•–æ—Ç—è –≤ –º–∞—Å–∫–µ —Ö–æ–¥–∏—Ç—å –∫–æ–Ω–µ—á–Ω–æ —Å—Ç—Ä–∞–Ω–Ω–æ–≤–∞—Ç–æ, –≤—ã–≥–ª—è–∂—É —Å—Ç—Ä–∞–Ω–Ω–æ, –º—É–∂–∞ –∑–∞–ø—É–≥–∏–≤–∞–ª–∞ –¥–æ–≤–æ–ª—å–Ω–æ –¥–æ–ª–≥–æ)))', id="1")
    print(filter.filter(doc))
